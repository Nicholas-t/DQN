{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bdfff4f",
   "metadata": {},
   "source": [
    "# Deep Reinforcement learning\n",
    "\n",
    "Author : Nicholas Setijabudiharsa\n",
    "\n",
    "TSE - Econometrics and Statistics : Data science for social science\n",
    "\n",
    "2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeeb09c",
   "metadata": {},
   "source": [
    "## 1 Reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41baad6b",
   "metadata": {},
   "source": [
    "### 1.1 Definitions and  Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd281d69",
   "metadata": {},
   "source": [
    "In order to better understand the idea of reinforcement learning, lets take an example of a game of chess. Theoritically speaking, one can create an brute force algorithm that analyze all possible states of the game and takes into account all the possible states that can come after the current state, and take the action that give the most probability of winning at the end. However, this method is very inefficient. In 2021, [John Tromp](https://github.com/tromp/ChessPositionRanking) did an analysis and found that there are about $4.8 x 10^{44}$ possible combinations of legal chess positions in chess. For perspective, here is the full form of that number\n",
    "\n",
    "$$480000000000000000000000000000000000000000000$$\n",
    "\n",
    "So for sure, this process is very inefficient. This is where Reinforcement learning comes in handy.\n",
    "\n",
    "\n",
    "\n",
    "Reinforcement learning is the process optimizing an agent's action $a$ of a given state $s$ by optimizing it's policy $\\pi(s, a)$ where\n",
    "$$\n",
    "\\pi(s, a) = Pr(a=a | s=s)\n",
    "$$\n",
    "\n",
    "This allows us to define a notion of Value for a given state for a given policy which will be the expected discounted future rewards or in mathematical terms\n",
    "\n",
    "$$\n",
    "V_\\pi(s) = E(\\sum_t\\gamma^t r_t |s_0=s)\n",
    "$$\n",
    "\n",
    "with $\\gamma$ to be the discount rate.\n",
    "\n",
    "\n",
    "With our given framework we can now understand that the goal of Reinforcement learning is a **maximization problem** where we **optimize** our agent's policy to **maximize** future rewards.\n",
    "\n",
    "Generally we also normally assume our model is based on a markovian decision process, where we dont take the next steps with certainty but instead given a current state $s_t$, we assume that $s_{t+1}$ adopts a certain probability distribution. For example, any games that includes rolling a dice.\n",
    "\n",
    "An environment where we get an information about the reward after every action an agent take, is called **Dense** rewards, on the other hand, environment such as playing a simple game of chess where we only receive rewards at the end of the game (whether we win or loses), is called **Sparse** rewards, naturally, the denser the rewards, the more efficient our data will be and therefore faster our learning will be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24824c8d",
   "metadata": {},
   "source": [
    "## 2 Deep Reinforcement Learning [draft only](https://www.youtube.com/watch?v=wDVteayWWvU&ab_channel=SteveBrunton)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd339083",
   "metadata": {},
   "source": [
    "### 2.1 Deep policy network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7378bf",
   "metadata": {},
   "source": [
    "Takes input state and returns policy $\\pi_\\theta(s, a)$ where $\\theta$ is weight of our NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f58141f",
   "metadata": {},
   "source": [
    "### 2.2 Deep Q Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68675b40",
   "metadata": {},
   "source": [
    "## 3 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3c3507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c936e33f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e961b91e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bed28f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
